{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to replicate performance\n",
    "In this notebook, we replicate the machine learning comparison performed in the paper. Unfortunately, we are not able to replicate the performance reported. In fact, only two methods are able to consistently outperform random guessing. Moreover, these two methods show an AUC of ~0.55 at best, far below the performance found in the paper of AUC 0.79."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, ElasticNet\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patient class labels\n",
    "case_control_df = pd.read_excel('../controlcase.xlsx')\n",
    "patient_id_to_case = case_control_df[['DummyID', 'Class']].set_index('DummyID')['Class'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature summary statistic data\n",
    "data_path = pathlib.Path('../data/secure/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary path to features directory in order to extract names\n",
    "# of relevant features\n",
    "feature_path = next(data_path.glob('*/*/*/feature/'))\n",
    "\n",
    "feature_names = list()\n",
    "for filename in feature_path.glob('*.nii.gz'):\n",
    "    feature_name = re.search('(?<=_norm_).+(?=\\.nii\\.gz)', filename.name).group()\n",
    "    feature_names.append(feature_name)\n",
    "    \n",
    "feature_names = sorted(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data into feature vectors\n",
    "# Note: Median was not used in the paper\n",
    "feature_vectors = []\n",
    "classes = []\n",
    "\n",
    "for sheet_path in data_path.glob('*/*/sheet/'):\n",
    "    patient_id = int(sheet_path.parent.parent.name)\n",
    "    \n",
    "    mean_path = next(sheet_path.glob('*_mean.csv'))\n",
    "#     median_path = next(sheet_path.glob('*_median.csv'))\n",
    "    std_path = next(sheet_path.glob('*_std.csv'))\n",
    "    \n",
    "    mean_df = pd.read_csv(mean_path)\n",
    "#     median_df = pd.read_csv(median_path)\n",
    "    std_df = pd.read_csv(std_path)\n",
    "    \n",
    "    mean_df.dropna(inplace=True)\n",
    "    std_df.dropna(inplace=True)\n",
    "    if len(mean_df) == 0 or len(std_df) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Order feature vector as mean, std with all values in the same order\n",
    "    feat = np.concatenate((\n",
    "        mean_df.iloc[0,:][feature_names].values.flatten(), \n",
    "        std_df.iloc[0,:][feature_names].values.flatten()\n",
    "    ))\n",
    "    feature_vectors.append(feat)\n",
    "    classes.append(patient_id_to_case[patient_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data into two numpy arrays\n",
    "x = np.zeros((len(feature_vectors), len(feature_vectors[0])))\n",
    "for i, v in enumerate(feature_vectors):\n",
    "    x[i] = v\n",
    "y = np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split. Verify that its roughly stratified.\n",
    "# Can't shuffle because some patients have multiple samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18900343642611683"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2328767123287671"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum() / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive machine learning methods\n",
    "The following pipeline gave the best and most robust performance: [min/max scaling, PCA, gradient boosting classifier]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Accuracy: 0.771689497716895\n",
      "AUC: 0.5098039215686274\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "Accuracy: 0.410958904109589\n",
      "AUC: 0.5273109243697479\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Accuracy: 0.7625570776255708\n",
      "AUC: 0.5243347338935573\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/miniconda3/envs/cis537/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.776255707762557\n",
      "AUC: 0.5469187675070027\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "Accuracy: 0.7168949771689498\n",
      "AUC: 0.4877450980392157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [LogisticRegression(), SGDClassifier(), \n",
    "              RandomForestClassifier(n_estimators=20), \n",
    "              GradientBoostingClassifier(), AdaBoostClassifier()]:\n",
    "    print(model)\n",
    "    mmscaler = MinMaxScaler()\n",
    "    scaler = PCA()\n",
    "    pipe = Pipeline([('minmax', mmscaler), ('PCA', scaler), ('ML model', model)])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    accuracy = pipe.score(X_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    preds = pipe.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    print(f\"AUC: {auc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net feature selection\n",
    "Unlike in the paper, elastic net narrowed to 10 features. Only after reducing the L1 penalty by 50% do 12 features appear. \n",
    "\n",
    "See regressor docs:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3294124516672958e-07 cooccurrence_win_97_sliding_97_numbin_128_offset_17_clusterProminence\n",
      "2.2654627417148872e-05 cooccurrence_win_97_sliding_97_numbin_128_offset_17_clusterShade\n",
      "-2.2188121038542043e-08 cooccurrence_win_97_sliding_97_numbin_128_offset_17_correlation\n",
      "-8.120668485534963e-05 cooccurrence_win_97_sliding_97_numbin_128_offset_17_energy\n",
      "0.00013606320831356636 cooccurrence_win_97_sliding_97_numbin_128_offset_17_entropy\n",
      "4.8163651208789366e-05 cooccurrence_win_97_sliding_97_numbin_128_offset_17_haralickCorrelation\n",
      "-1.7441680255954657e-07 cooccurrence_win_97_sliding_97_numbin_128_offset_17_inertia\n",
      "2.3826922148237065e-05 cooccurrence_win_97_sliding_97_numbin_128_offset_17_inverseDifferenceMoment\n",
      "-2.952618964204289e-09 edgeenhance_win_97_sliding_97_Eta_10_epsi_10_radius_edge_8_radius_8_edge_enhance\n",
      "-3.658555851167091e-05 graylevel_win_97_sliding_97_numbin_128_5th\n",
      "0.00025342873072403005 graylevel_win_97_sliding_97_numbin_128_5thmean\n",
      "2.0116432573701287e-05 graylevel_win_97_sliding_97_numbin_128_95th\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "elnet = ElasticNet(alpha=0.5)\n",
    "elnet.fit(X_train, y_train)\n",
    "\n",
    "selected_coefs = list()\n",
    "for i, coef in enumerate(elnet.coef_):\n",
    "    if np.abs(coef) > 1e-9:\n",
    "        if i < len(feature_names):\n",
    "            name = feature_names[i]\n",
    "            print(coef, name)\n",
    "        else:\n",
    "            name = feature_names[i % len(feature_names)]\n",
    "            print(coef, name)\n",
    "        selected_coefs.append(name)\n",
    "        \n",
    "print(len(selected_coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using elastic net selected features\n",
    "\n",
    "No method is able to replicate the performance of the MinMax/PCA/GradientBoosting pipeline above, which used all available features as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut down train/test data to those features selected by elastic net\n",
    "X_train = X_train[:, np.abs(elnet.coef_) > 1e-9]\n",
    "X_test = X_test[:, np.abs(elnet.coef_) > 1e-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Accuracy: 0.7671232876712328\n",
      "AUC: 0.5\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "Accuracy: 0.7671232876712328\n",
      "AUC: 0.5\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Accuracy: 0.7534246575342466\n",
      "AUC: 0.5047268907563025\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Accuracy: 0.7579908675799086\n",
      "AUC: 0.507703081232493\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/miniconda3/envs/cis537/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7579908675799086\n",
      "AUC: 0.507703081232493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ML models without any scaling or preprocessing\n",
    "for model in [LogisticRegression(), SGDClassifier(), \n",
    "              RandomForestClassifier(n_estimators=20), \n",
    "              GradientBoostingClassifier(), AdaBoostClassifier()]:\n",
    "    \n",
    "    print(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    print(f\"AUC: {auc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Accuracy: 0.7671232876712328\n",
      "AUC: 0.5\n",
      "\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "Accuracy: 0.2328767123287671\n",
      "AUC: 0.5\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Accuracy: 0.7625570776255708\n",
      "AUC: 0.49702380952380953\n",
      "\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Accuracy: 0.7579908675799086\n",
      "AUC: 0.507703081232493\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/miniconda3/envs/cis537/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7579908675799086\n",
      "AUC: 0.507703081232493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ML pipeline with MinMaxScaler scaling\n",
    "for model in [LogisticRegression(), SGDClassifier(), \n",
    "              RandomForestClassifier(n_estimators=20), \n",
    "              GradientBoostingClassifier(), AdaBoostClassifier()]:\n",
    "    print(model)\n",
    "    scaler = MinMaxScaler()\n",
    "    pipe = Pipeline([('scale', scaler), ('ML model', model)])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    accuracy = pipe.score(X_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    preds = pipe.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    print(f\"AUC: {auc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cis537]",
   "language": "python",
   "name": "conda-env-cis537-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
